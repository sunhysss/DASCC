{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d8d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script does spatial whitening, and cc in frequency domain\n",
    "# Yan Yang 2022-07-10\n",
    "# Haoyuan Sun 2025-06-30\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import h5py\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from func_PyCC import *\n",
    "from tqdm import tqdm\n",
    "from scipy.io import savemat\n",
    "from fstack import Fstack\n",
    "\n",
    "def between(lower, upper, array, mode=2):\n",
    "    \"\"\"\n",
    "    模拟 MATLAB 的 between 函数。\n",
    "    mode=2 表示包含边界。\n",
    "    \"\"\"\n",
    "    if mode == 2:\n",
    "        return np.where((array >= lower) & (array <= upper))[0]\n",
    "    else:\n",
    "        return np.where((array > lower) & (array < upper))[0]\n",
    "\n",
    "def pair_to_index(i, j, n):\n",
    "    \"\"\"将(i, j)形式的互相关对转换为连续线性索引\"\"\"\n",
    "    index = (i * (2 * n - i + 1)) // 2 + (j - i)  # 生成索引\n",
    "    return index\n",
    "\n",
    "def index_to_pair(index, n):\n",
    "    \"\"\"通过线性索引恢复(i, j)形式的互相关对\"\"\"\n",
    "    row = 0\n",
    "    while index >= (n - row):\n",
    "        index -= (n - row)\n",
    "        row += 1\n",
    "    col = row + index\n",
    "    return row, col\n",
    "\n",
    "# 计算当前排列和前一个排列的线性索引\n",
    "def reuse_index(step, n):\n",
    "    index_previous = []  # 存储前一个排列的线性索引\n",
    "    index_current = []   # 存储当前排列的线性索引\n",
    "    index_new = [] # 需要新计算的索引\n",
    "\n",
    "    # 计算当前排列和前一个排列的互相关对索引\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            # 计算当前排列和前一个排列的线性索引\n",
    "            index_current_i = pair_to_index(i + step, j + step, n)\n",
    "            index_previous_i = pair_to_index(i, j, n)\n",
    "\n",
    "            # 如果当前索引不存在于 uxt_cn2 中，说明需要新计算\n",
    "            if j >= n - step:\n",
    "                index_new.append(index_previous_i)\n",
    "            else:\n",
    "                index_current.append(index_current_i)\n",
    "                index_previous.append(index_previous_i)\n",
    "\n",
    "    # 返回索引数组\n",
    "    return index_current, index_previous, index_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a1f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1112701/3376070646.py:5: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = [x.strftime('%Y%m%d%H') for x in pd.date_range(start='2023-12-23 15:00', end='2024-01-06 23:00', freq='1H')]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 指定处理周期\n",
    "date_range = '231223_240120'\n",
    "dates = [x.strftime('%Y%m%d%H') for x in pd.date_range(start='2023-12-23 15:00', end='2024-01-06 23:00', freq='1H')]\n",
    "\n",
    "# 指定目录路径\n",
    "path_preprocessed = f'/media/Data/hySun/2025_hzdf/S-preprocessed/{date_range}'\n",
    "output_CC = './CC/'\n",
    "if not os.path.exists(output_CC):\n",
    "    os.mkdir(output_CC)\n",
    "\n",
    "begin_channel = 0\n",
    "end_channel = 2416\n",
    "npair = end_channel - begin_channel + 1\n",
    "#############\n",
    "\n",
    "f1, f2 = 1, 30 # frequency band in spectral whitening\n",
    "fs = 100 # sampling frequency\n",
    "is_spectral_whitening = True\n",
    "window_freq = 0 # 0 means aggresive spectral whitening, otherwise running mean\n",
    "# max_lag = 30 # in sec, the time lag of the output CC\n",
    "max_lag = 3 # in sec, the time lag of the output CC\n",
    "npts_lag = int(max_lag*fs)\n",
    "xcorr_seg = 60 # in sec, the length of the segment to compute CC, slightly larger than max_lag is good\n",
    "npts_seg = int(xcorr_seg*fs)\n",
    "\n",
    "device = \"cuda\"\n",
    "gpu_ids = [0] # GPU device id\n",
    "# gpu_ids = [0,1] # GPU device id\n",
    "npair_chunk = 52 # depends on # of channels, sampling frequency, and xcorr_seg, needs to be adaptive\n",
    "npair_chunk_sub = 26 # depends on # of channels, sampling frequency, and xcorr_seg, needs to be adaptive\n",
    "overlap = 0.5  # 设置重叠度为50%\n",
    "step = int(npair_chunk * (1 - overlap))  # 计算步长\n",
    "step_sub = int(npair_chunk_sub * (1 - overlap))\n",
    "# 更新 nchunk，以确保覆盖所有通道对\n",
    "nchunk = int(np.ceil((npair - npair_chunk) / step))\n",
    "nchunk_sub = int(np.ceil((npair - npair_chunk_sub) / step_sub))\n",
    "\n",
    "# bin-stack\n",
    "bin = 2\n",
    "bin_x =[]\n",
    "for i in range(npair_chunk):\n",
    "    x = range(npair_chunk-i)\n",
    "    bin_x.append(x)\n",
    "x_offset = np.concatenate(bin_x, axis=0)\n",
    "x_min = np.round(np.min(x_offset) / bin) * bin\n",
    "x_max = np.round(np.max(x_offset) / bin) * bin\n",
    "x_stack = np.arange(x_min, x_max + 2 * bin, bin)\n",
    "ntrace_stack = len(x_stack)\n",
    "index_list = []\n",
    "for i in range(ntrace_stack):\n",
    "    # 计算范围内的索引\n",
    "    lower_bound = x_stack[i] - bin / 2\n",
    "    upper_bound = x_stack[i] + bin / 2\n",
    "    index = between(lower_bound, upper_bound, x_offset, mode=2)\n",
    "    index_list.append(index)\n",
    "\n",
    "bin_x_sub =[]\n",
    "x_offset_sub_to_all = []\n",
    "for i in range(npair_chunk_sub):\n",
    "    x1 = range(npair_chunk_sub-i)\n",
    "    bin_x_sub.append(x1)\n",
    "x_offset_sub = np.concatenate(bin_x_sub, axis=0)\n",
    "x_min = np.round(np.min(x_offset_sub) / bin) * bin\n",
    "x_max = np.round(np.max(x_offset_sub) / bin) * bin\n",
    "x_stack_sub = np.arange(x_min, x_max + 2 * bin, bin)\n",
    "ntrace_stack_sub = len(x_stack_sub)\n",
    "for i in range(npair_chunk_sub):\n",
    "    i0 = i * np.ones((npair_chunk_sub-i))\n",
    "    j0 = range(i,npair_chunk_sub)\n",
    "    for j in range(len(i0)):\n",
    "        x_offset_sub_to_all.append(pair_to_index(i0[j], j0[j], npair_chunk))\n",
    "x_offset_sub_to_all = np.array(x_offset_sub_to_all)\n",
    "index_list0 = []\n",
    "index_list1 = []\n",
    "index_list2 = []\n",
    "for i in range(ntrace_stack_sub):\n",
    "    # 计算范围内的索引\n",
    "    lower_bound = x_stack_sub[i] - bin / 2\n",
    "    upper_bound = x_stack_sub[i] + bin / 2\n",
    "    index0 = between(lower_bound, upper_bound, x_offset_sub, mode=2)\n",
    "    index0 = index0[index0 < x_offset_sub_to_all.shape[0]]\n",
    "    index0 = x_offset_sub_to_all[index0]\n",
    "    index1 = np.zeros((index0.shape))\n",
    "    index2 = np.zeros((index0.shape))\n",
    "    for j in range(len(index0)):\n",
    "        pair1, pair2 = index_to_pair(index0[j], npair_chunk)\n",
    "        index1[j] = pair_to_index(pair1+step_sub, pair2+step_sub, npair_chunk)\n",
    "        index2[j] = pair_to_index(pair1+step_sub+step_sub, pair2+step_sub+step_sub, npair_chunk)\n",
    "    index0 = index0.astype(int)\n",
    "    index1 = index1.astype(int)\n",
    "    index2 = index2.astype(int)\n",
    "    index_list0.append(index0)\n",
    "    index_list1.append(index1)\n",
    "    index_list2.append(index2)\n",
    "    \n",
    "index_current, index_previous, index_new = reuse_index(step, npair_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c41bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/345 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023122315已存在。\n",
      "2023122316已存在。\n",
      "2023122317已存在。\n",
      "2023122318已存在。\n",
      "2023122319已存在。\n",
      "2023122320已存在。\n",
      "2023122321已存在。\n",
      "2023122322已存在。\n",
      "2023122323已存在。\n",
      "2023122400已存在。\n",
      "2023122401已存在。\n",
      "2023122402已存在。\n",
      "2023122403已存在。\n",
      "2023122404已存在。\n",
      "2023122405已存在。\n"
     ]
    }
   ],
   "source": [
    "model_conf = {\n",
    "    \"is_spectral_whitening\": is_spectral_whitening,\n",
    "    \"whitening_params\": [fs, window_freq, f1, f2],\n",
    "}\n",
    "model = Torch_cross_correlation(**model_conf)\n",
    "model = nn.DataParallel(model, device_ids=gpu_ids).to(device)\n",
    "model.eval()\n",
    "\n",
    "#%%\n",
    "for idate in tqdm(dates):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    output_file_path = f'{output_CC}/{date_range}/cn2_{npair_chunk}channel_{overlap}overlap/hour/'\n",
    "    os.makedirs(output_file_path, exist_ok=True)\n",
    "    output_file_tmp = f'{output_file_path}{idate}.mat'\n",
    "\n",
    "    output_file_path_sub = f'{output_CC}/{date_range}/cn2_{npair_chunk_sub}channel_{overlap}overlap/hour/'\n",
    "    os.makedirs(output_file_path_sub, exist_ok=True)\n",
    "    output_file_tmp_sub = f'{output_file_path_sub}{idate}.mat'\n",
    "\n",
    "    if os.path.exists(output_file_tmp):\n",
    "        print(f\"{idate}已存在。\")\n",
    "        continue\n",
    "\n",
    "    filelist = glob(os.path.join(path_preprocessed,idate+'*h5'))\n",
    "    filelist.sort()\n",
    "\n",
    "    if len(filelist) == 0:\n",
    "        print(f'{idate}: no file')\n",
    "        continue\n",
    "\n",
    "    ccall = torch.zeros((nchunk*ntrace_stack, int(max_lag * fs * 2 + 1), len(filelist)), device=device)\n",
    "    ccall_sub = torch.zeros((nchunk_sub*ntrace_stack_sub, int(max_lag * fs * 2 + 1), len(filelist)), device=device)\n",
    "\n",
    "    data_hour = []  # 用列表收集每分钟数据\n",
    "\n",
    "    for ifile in filelist:\n",
    "        try:\n",
    "            with h5py.File(ifile, 'r') as fid:\n",
    "                data = fid['SeisData'][:]  # 假设 data.shape 是 (n_channel, n_sample_per_minute)\n",
    "                data_hour.append(data[np.newaxis, ...])  # 增加新维度，变成 (1, n_channel, n_sample)\n",
    "        except Exception as e:\n",
    "            print(f\"跳过文件 {ifile}，读取时出错：{e}\")\n",
    "            continue\n",
    "\n",
    "    data_hour = np.concatenate(data_hour, axis=0)  # 变成 (n_minute, n_channel, n_sample)\n",
    "\n",
    "    for file_idx in range(len(filelist)):\n",
    "        \n",
    "        data = torch.from_numpy(data_hour[file_idx]).to(torch.float32).to(device)\n",
    "\n",
    "        for ichunk in range(nchunk):\n",
    "            start_idx = begin_channel + ichunk * step\n",
    "            end_idx = start_idx + npair_chunk\n",
    "\n",
    "            if ichunk == 0:\n",
    "                data1_parts = []\n",
    "                for i in range(npair_chunk):\n",
    "                    row = data[start_idx + i, :].unsqueeze(0).repeat(npair_chunk - i, 1)\n",
    "                    data1_parts.append(row)\n",
    "                data1 = torch.cat(data1_parts, dim=0).reshape(-1, npts_seg)\n",
    "                data2_parts = []\n",
    "                for i in range(npair_chunk):\n",
    "                    slice_data = data[start_idx + i : end_idx, :]\n",
    "                    data2_parts.append(slice_data)\n",
    "                data2 = torch.cat(data2_parts, dim=0).reshape(-1, npts_seg)\n",
    "\n",
    "                cc = model(data1, data2)\n",
    "                cc = cc[:,npts_seg - npts_lag - 1:npts_lag - npts_seg + 1]\n",
    "                uxt_cn2 = torch.zeros((ntrace_stack, cc.shape[1]), device=device)\n",
    "                for i in range(ntrace_stack):\n",
    "                    uxt_cn2[i,:] = torch.sum(cc[index_list[i],:], dim=0)\n",
    "                ccall[ntrace_stack * ichunk: ntrace_stack * (ichunk + 1), :, file_idx] = uxt_cn2\n",
    "\n",
    "                uxt_cn2_sub0 = torch.zeros((ntrace_stack_sub, cc.shape[1]), device=device)\n",
    "                uxt_cn2_sub1 = torch.zeros((ntrace_stack_sub, cc.shape[1]), device=device)\n",
    "                uxt_cn2_sub2 = torch.zeros((ntrace_stack_sub, cc.shape[1]), device=device)\n",
    "                for i in range(ntrace_stack_sub):\n",
    "                    uxt_cn2_sub0[i,:] = torch.sum(cc[index_list0[i],:], dim=0)\n",
    "                    uxt_cn2_sub1[i,:] = torch.sum(cc[index_list1[i],:], dim=0)\n",
    "                    uxt_cn2_sub2[i,:] = torch.sum(cc[index_list2[i],:], dim=0)\n",
    "                ccall_sub[ntrace_stack_sub * 2 * ichunk: ntrace_stack_sub * (2 * ichunk + 1), :, file_idx] = uxt_cn2_sub0\n",
    "                ccall_sub[ntrace_stack_sub * (2 * ichunk + 1): ntrace_stack_sub * (2 * ichunk + 2), :, file_idx] = uxt_cn2_sub1\n",
    "                ccall_sub[ntrace_stack_sub * (2 * ichunk + 2): ntrace_stack_sub * (2 * ichunk + 3), :, file_idx] = uxt_cn2_sub2\n",
    "\n",
    "            else:\n",
    "                cc_reuse = torch.zeros((cc.shape), device=device)\n",
    "                cc_reuse[index_current,:] = cc[index_previous,:]\n",
    "\n",
    "                data1_parts = []\n",
    "                for i in range(npair_chunk):\n",
    "                    row = data[start_idx + i, :].unsqueeze(0).repeat(npair_chunk - i, 1)\n",
    "                    data1_parts.append(row)\n",
    "                data1 = torch.cat(data1_parts, dim=0).reshape(-1, npts_seg)\n",
    "                data1 = data1[index_new,:]\n",
    "                data2_parts = []\n",
    "                for i in range(npair_chunk):\n",
    "                    slice_data = data[start_idx + i : end_idx, :]\n",
    "                    data2_parts.append(slice_data)\n",
    "                data2 = torch.cat(data2_parts, dim=0).reshape(-1, npts_seg)\n",
    "                data2 = data2[index_new,:]\n",
    "\n",
    "                cc = model(data1, data2)\n",
    "                cc = cc[:,npts_seg - npts_lag - 1:npts_lag - npts_seg + 1]\n",
    "                cc_reuse[index_new,:] = cc\n",
    "                cc = cc_reuse\n",
    "                uxt_cn2 = torch.zeros((ntrace_stack, cc.shape[1]), device=device)\n",
    "                for i in range(ntrace_stack):\n",
    "                    uxt_cn2[i,:] = torch.sum(cc[index_list[i],:], dim=0)\n",
    "                ccall[ntrace_stack * ichunk: ntrace_stack * (ichunk + 1), :, file_idx] = uxt_cn2\n",
    "                \n",
    "                uxt_cn2_sub1 = torch.zeros((ntrace_stack_sub, cc.shape[1]), device=device)\n",
    "                uxt_cn2_sub2 = torch.zeros((ntrace_stack_sub, cc.shape[1]), device=device)\n",
    "                for i in range(ntrace_stack_sub):\n",
    "                    uxt_cn2_sub1[i,:] = torch.sum(cc[index_list1[i],:], dim=0)\n",
    "                    uxt_cn2_sub2[i,:] = torch.sum(cc[index_list2[i],:], dim=0)\n",
    "                ccall_sub[ntrace_stack_sub * (2 * ichunk + 1): ntrace_stack_sub * (2 * ichunk + 2), :, file_idx] = uxt_cn2_sub1\n",
    "                ccall_sub[ntrace_stack_sub * (2 * ichunk + 2): ntrace_stack_sub * (2 * ichunk + 3), :, file_idx] = uxt_cn2_sub2\n",
    "\n",
    "    ccall = Fstack(ccall, stack_flag=1, nu=1, dim=2, device=device).cpu().numpy()\n",
    "    ccall_sub = Fstack(ccall_sub, stack_flag=1, nu=1, dim=2, device=device).cpu().numpy()\n",
    "\n",
    "    savemat(output_file_tmp, {'data': ccall})\n",
    "    savemat(output_file_tmp_sub, {'data': ccall_sub})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
